Mobile edge computing (MEC) has emerged as a promising paradigm to enhance the computational capabilities of resource-constrained secondary devices (RCSDs) in proximity to prescheduled primary devices (PDs). In this context, we introduce a novel framework where an energy harvesting (EH)-enabled RCSD efficiently offloads computational tasks to an MEC server, while employing a cognitive radio-inspired non-orthogonal multiple access (CR-NOMA) scheme for efficient data transmission. The RCSD also harvests energy from the ambient radio frequency (RF) signals of the surrounding PDs. We propose a deep reinforcement learning (DRL)-based optimization strategy, specifically the deep deterministic policy gradient (DDPG) algorithm to minimize the service delay of the RCSD and optimize the time-sharing coefficient for harvesting energy when offloading computational tasks to the MEC server. This dynamic resource allocation strategy intelligently determines the duration for which RCSDs transmit data and allocate time for energy harvesting, thereby ensuring an optimal balance between computation offloading and energy sustainability. Simulations demonstrate the effectiveness of the proposed scheme in maximizing the utility of the RCSDs while minimizing the overall service delay of the RCSD. Index Terms-Mobile edge computing (MEC), cognitive radio-inspired non-orthogonal multiple access (CR-NOMA), deep reinforcement learning (DRL), and deep deterministic policy gradient (DDPG).
